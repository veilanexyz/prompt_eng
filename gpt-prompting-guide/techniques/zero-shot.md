# Промтинг Zero-Shot

Большие языковые модели (LLM), такие как {{ yagpt-name }}, настроены на выполнение инструкций и обучены на больших объемах данных. Масштабное обучение дает использовать модели в режиме Zero-shot, который предполагает, что имеющих у модели данных достаточно для решения определенных задач. Рассмотрим простой пример классификации текста с помощью техники Zero-shot.

**Промт**

```text
Классифицируйте текст как нейтральный, негативный или позитивный.

Текст: Я думаю, что отпуск был нормальным.
Настроение:
```

**Ответ**
```text
Нейтральный
```

В этом промте мы не предоставили модели никаких примеров текста с классификациями, но модель уже понимает, что такое «настроение».

Расширить возможности промтинга Zero-shot можно с помощью [настройки инструкций](https://arxiv.org/pdf/2109.01652) — дообучения моделей на наборах данных, описанных через инструкции. Также можно использовать [RLHF](https://arxiv.org/abs/1706.03741) — обучение с подкреплением на основе обратной связи от человека — для масштабирования настройки инструкций, где модель лучше подстраивается под человеческие предпочтения.

#### См. также {#see-also}

* [{#T}](few-shot.md)
* [{#T}](CoT.md)
* [{#T}](self-consistency.md)
* [{#T}](../../tutorials/yagpt-tuning.md)
