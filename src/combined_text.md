# Рекомендации по использованию {{ yagpt-name }}

Модель {{ yagpt-name }} позволяет эффективно решать различные задачи, связанные с обработкой текстов. В этом разделе приведены некоторые рекомендации, которые помогут вам максимально эффективно использовать возможности {{ yagpt-name }}.

## Разделение сложных задач {#tasks-division}

Сложные задачи лучше разбивать на несколько простых, если это позволяет допустимое время обработки запроса и бюджет.

Например, вы решаете задачу переписывания неформального текста в более формальный. Ваше понимание «формального текста» и понимание модели отличаются. Чтобы получать текст в ожидаемом виде, вы передаете в промт длинный список критериев того, что такое «формальный текст». С увеличением критериев модель будет придавать все меньше значимости каждому из них, и точность результатов будет падать.

Однако как большинство задач реального мира, так и вашу текущую тоже, можно разложить на несколько простых:

1. Продумайте последовательные шаги решения вашей задачи.
1. Составьте промты для каждого шага.
1. Отправляйте их в модель последовательно.

Например, вы можете составить шесть критериев формального текста, трансформировать текст с вниманием к первым трем критериям, а затем повторить операцию с вниманием к оставшимся. Чем короче и лаконичнее промт, тем выше точность работы модели и меньше различий в ответах на похожие запросы.

## Приведение ответа к конкретным форматам {#formatting}

Если вам нужна высокая стабильность генерации текста в определенном формате, вы можете вынести переформатирование текста в дополнительный шаг. Например, если вам нужно преобразовать текст из [Markdown](https://ru.wikipedia.org/wiki/Markdown), можно действовать следующим образом:

1. Сначала переписать ответ модели еще раз.

   ```text
   Перепиши json ниже в правильный формат, если есть ошибки (например использование ```json или ``` — это нужно удалить). Не выводи ничего лишнего.
   ```

1. Очистить текст от артефактов программными методами.
Например, следующий скрипт на Python использует библиотеку `markdown2` и регулярное выражение для постобработки ответа {{ yagpt-name }}:

   ```python
   import markdown2
   import re

   markdown_text = """
   # Заголовок
   **Жирный текст**
   *Курсивный текст*
   [Ссылка](http://example.com)
   ![Изображение](http://example.com/image.jpg)
   `Код`
   - Список
   """
   html = markdown2.markdown(markdown_text)
   plain_text = re.sub(r'<[^>]+>', '', html)  # Удаление HTML-тегов
   print(plain_text)
   ```

## Указание длины выходного текста {#result-length}

Для задания длины выходного текста укажите примерную длину ответа в промте. Например:

```text
"Перепиши текст так, чтобы он был длиной около 1000 символов."
```

Также вы можете ограничить длину текста в [токенах](../concepts/yandexgpt/tokens.md). Для этого укажите параметр `max_tokens` в запросе к {{ yagpt-name }}.

## Дообучение модели {#fine-tuning}

Для повышения точности работы в некоторых задачах можно [дообучить](../../datasphere/concepts/models/foundation-models.md) модель под конкретную задачу.

Так, модель можно обучить:
* пересказывать и изменять формулировки текстов;
* генерировать вопросы к тексту и ответы на них;
* форматировать ответы в определенной стилистике или формате;
* классифицировать тексты, обращения и диалоги;
* извлекать сущности из текста.

Дообучение не поможет или несущественно увеличит точность результатов, если вы хотите:
* заложить в модель новые знания;
* улучшить или изменить знания модели;
* заложить в модель новую доменную область;
* заложить в модель понимание новых терминов;
* научить модель анализировать сложные технические данные.

Дообучение {{ yagpt-name }} изменяет только последний слой модели. О том, как дообучить модель, см. руководство [{#T}](../tutorials/yagpt-tuning.md).
# Руководство по проектированию промтов

*Проектирование промтов* или *промпт-инжиниринг* — это относительно новая дисциплина, направленная на разработку и оптимизацию запросов для эффективного использования языковых моделей (_Language models, LM_) в широком спектре приложений и исследовательских задач. Навыки создания промтов помогают лучше понять возможности и ограничения больших языковых моделей (_Large language models, LLM_), таких как {{ yagpt-name }}.

Проектирование промтов предполагает не только разработку и создание запросов. Оно также охватывает широкий спектр навыков и техник, полезных для взаимодействия с LLM и *создания новых решений* на их основе.

Исследователи используют инжиниринг промтов, чтобы расширить возможности LLM в решении различных задач, будь то ответы на вопросы или выполнение арифметических рассуждений. Разработчики применяют эти техники для создания надежных и эффективных методов взаимодействия с LLM и другими инструментами.

Данное руководство по проектированию запросов содержит все последние научные статьи, передовые техники промт-инжиниринга, учебные материалы, руководства по использованию конкретных моделей, лекции, справочные материалы, новые возможности LLM и инструменты, связанные с созданием запросов.

Все примеры протестированы на модели {{ yagpt-name }}. Модель использует параметры по умолчанию, такие как `temperature=0.3`. Промты также должны работать с другими моделями, обладающими схожими возможностями, но ответы моделей могут различаться.

## О чем это руководство? {#chapters}

* На странице [{#T}](popular-problems-solving.md) мы разобрали часто возникающие проблемы и методы их решения.
* Раздел [Введение в инжиниринг промтов](introduction/basics.md) содержит базовые знания о проектировании промтов и взаимодействии с LLM. Обязательно прочитайте ее, если до этого никогда не работали с большими языковыми моделями или только начинаете свое знакомство с ними.
* Раздел [{#T}](techniques/about.md) рассказывает о продвинутых техниках проектирования промтов.
{% if audience == "draft" %} * Раздел [{#T}](architectures/about.md) описывает архитектуры сервисов, которые можно реализовывать с использованием {{ foundation-models-full-name }}.{% endif %}

#### Вебинар «Мастерство промт-инжиниринга» {#webinar}

@[youtube](https://www.youtube.com/embed/ttZSnuO-xXI?si=sB0-kxs0EIrhfILx)
# Примеры промтов

В этом разделе мы рассмотрим больше примеров использования промтов для выполнения различных задач с помощью {{ yagpt-name }} и познакомимся с ключевыми концепциями. Часто лучший способ изучить концепции — это пройти через примеры. Примеры ниже иллюстрируют, как можно использовать хорошо продуманные промты для более продуктивной работы с моделью {{ yagpt-name }}.

Типы задач, которые мы разберем на этой странице:

* [Краткий пересказ текста](#text-summarization)
* [Извлечение информации](#information-extraction)
* [Ответы на вопросы](#question-answering)
* [Классификация текста](#text-classification)
* [Разговор](#conversation)
* [Логические рассуждения](#reasoning)

## Краткий пересказ текста {#text-summarization}

Одной из стандартных задач в области генерации естественного языка является краткий пересказ, или суммаризация текста. Суммаризация текста может охватывать множество различных форматов и областей. Одним из самых перспективных применений языковых моделей является возможность суммаризовать статьи и концепции в быстрые и легкие для чтения резюме. Попробуем выполнить базовую задачу суммаризации с помощью промтов:

**Промт**

```text
Антибиотики — это тип лекарств, используемых для лечения бактериальных инфекций. Они работают, убивая бактерии или предотвращая их размножение, что позволяет иммунной системе организма бороться с инфекцией. Антибиотики обычно принимаются внутрь в виде таблеток, капсул или жидких растворов, а иногда вводятся внутривенно. Они не эффективны против вирусных инфекций, и их неправильное использование может привести к устойчивости к антибиотикам.

Объясни вышеуказанное в одном предложении:
```

**Ответ**

```text
Антибиотики — это лекарства, используемые для лечения бактериальных инфекций, убивая бактерии или предотвращая их размножение, но они не эффективны против вирусов, и их чрезмерное использование может привести к устойчивости к антибиотикам.
```

Модель попыталась суммаризовать абзац в одно предложение. Попробуйте поэкспериментировать с инструкциями, чтобы улучшить результаты.

## Извлечение информации {#information-extraction}

Хотя языковые модели обучены выполнять задачи генерации естественного языка и связанные с ними задачи, они также способны выполнять классификацию и ряд других задач обработки естественного языка (natural language processing, NLP). Рассмотрим промт, который извлекает информацию из текста:

**Промт**

```text
В заявлениях об авторском вкладе и благодарностях в научных статьях следует четко и конкретно указывать, использовали ли авторы и в какой степени технологии ИИ, такие как YandexGPT, при подготовке своей рукописи и анализа. Они также должны указывать, какие LLM использовались. Это позволит редакторам и рецензентам более тщательно проверять рукописи на предмет возможных предвзятостей, неточностей и неправильного указания источников. Аналогично, научные журналы должны быть прозрачны в отношении использования LLM, например, при выборе представленных рукописей.

Укажите продукт на базе большой языковой модели, упомянутый в абзаце выше:
```

**Ответ**

```text
Продукт на базе большой языковой модели, упомянутый в абзаце выше, — это YandexGPT.
```

Существует множество способов улучшить результаты выше, но даже такие простые запросы могут решить множество задач.

## Ответы на вопросы {#question-answering}

Один из лучших способов заставить модель отвечать на конкретные вопросы — улучшить формат промта. Это можно сделать с помощью[элементов](elements.md) промта, которые сочетают инструкции, контекст, ввод и индикаторы вывода для получения улучшенных результатов. Они не являются обязательными, но позволяют значительно конкретизировать инструкции для модели. Например:

**Промт**

```text
Ответьте на вопрос на основе приведенного ниже контекста. Держите ответ коротким и лаконичным. Ответьте "Не уверен в ответе", если не уверены в ответе.

Контекст: Теплизумаб берет свое начало в фармацевтической компании Ortho Pharmaceutical в Нью-Джерси. Там ученые создали раннюю версию антитела, названную OKT3. Изначально полученное от мышей, это молекула могла связываться с поверхностью Т-клеток и ограничивать их способность убивать клетки. В 1986 году оно было одобрено для предотвращения отторжения органов после пересадки почки, став первым терапевтическим антителом, разрешенным для использования у людей.

Вопрос: Откуда изначально было получено OKT3?
```

**Ответ**

```text
От мышей.
```


## Классификация текста {#text-classification}

До сих пор вы использовали простые инструкции, которых будет недостаточно для сложных случаев. Для задач классификации могут понадобиться конкретные примеры:

**Промт**

```text
Классифицируйте текст как нейтральный, негативный или позитивный.

Текст: Я думаю, что еда была нормальной.
Настроение:
```

**Ответ**

```text
Нейтральное
```

Модель ответила верно, но предположим, что мы хотим получить ответ со строчной буквы. Существует несколько способов сделать это. Важна конкретность, поэтому чем больше информации вы можете предоставить в промте, тем лучше результаты. Попробуем добавить в запрос пример:

**Промт**

```text
Классифицируйте текст как нейтральный, негативный или позитивный.

Текст: Я думаю, что отпуск был нормальным.
Настроение: нейтральное

Текст: Я думаю, что еда была нормальной.
Настроение: 
```

**Ответ**

```text
нейтральное
```

На этот раз модель вернула конкретную метку «нейтральное». Приведенный пример помог модели быть конкретной в своем ответе.

## Разговор {#conversation}

Одной из самых интересных вещей, которые вы можете достичь с помощью инженерии промтов, является указание системе LLM, как себя вести, каковы ее намерения и идентичность. Это особенно полезно при создании разговорных систем, таких как чат-боты для обслуживания клиентов.

Например, создадим разговорную систему, которая способна генерировать более технические и научные ответы на вопросы. Обратите внимание, как через инструкцию модели задается нужный стиль ответов. Это иногда называют *ролевым промтингом*. Например:

**Промт**

```text
Следующий диалог происходит с ИИ-ассистентом по исследованиям. Тон ассистента технический и научный.

Человек: Привет, кто ты?
ИИ: Приветствую! Я ИИ-ассистент по исследованиям. Как я могу помочь вам сегодня?
Человек: Можешь рассказать мне о создании черных дыр?
ИИ:
```

**Ответ**

```text
Конечно! Черные дыры — это области пространства-времени, где гравитационная сила настолько сильна, что ничто, даже свет, не может из нее вырваться. Они образуются, когда очень массивная звезда умирает и ее ядро коллапсирует, образуя сингулярность бесконечной плотности. Интенсивная гравитация сингулярности притягивает всю материю и излучение вокруг нее, создавая черную дыру.
```

Если полученные ответы кажутся слишком сложными, можно попробовать их упростить:

**Промт**

```text
Следующий диалог происходит с ИИ-ассистентом по исследованиям. Ответы ассистента должны быть понятны даже ученикам начальной школы.

Человек: Привет, кто ты?
ИИ: Приветствую! Я ИИ-ассистент по исследованиям. Как я могу помочь вам сегодня?
Человек: Можешь рассказать мне о создании черных дыр?
ИИ:
```

**Ответ**

```text
Конечно! Черные дыры образуются, когда очень массивная звезда заканчивает свое топливо и коллапсирует. Гравитация звезды настолько сильна, что ничто, даже свет, не может вырваться из ее притяжения. Это создает точку в пространстве-времени с интенсивной гравитационной силой, называемую черной дырой.
```

Заметен прогресс. Если вы добавите больше примеров, вы можете получить результаты.

## Логические рассуждения {#reasoning}

Одними из самых сложных задач для LLM сегодня являются те, которые требуют какой-либо формы рассуждений. Логические рассуждения — одна из самых интересных областей из-за типов сложных приложений, которые могут возникнуть из LLM. Существующие модели до сих пор испытывают трудности с выполнением задач, требующих рассуждений, поэтому это требует еще более [продвинутых техник](llm-settings.md) инженерии промтов. Рассмотрим базовый пример из арифметики:

**Промт**

```text
Нечетные числа в этой группе складываются в четное число: 15, 32, 5, 13, 82, 7, 1.

**Ответ**
```text
Нет, нечетные числа в этой группе складываются в нечетное число: 119.
```

Модель ответила неверно. Попробуем улучшить промт дополнительной инструкцией:

**Промт**

```text
Нечетные числа в этой группе складываются в четное число: 15, 32, 5, 13, 82, 7, 1.

Решите, разбив проблему на шаги. Сначала определите нечетные числа, сложите их и укажите, является ли результат четным или нечетным.
```

**Ответ**

```text
Нечетные числа: 15, 5, 13, 7, 1
Сумма: 41
41 — нечетное число.
```

Теперь ответ верный, но модель все еще может ошибиться на других примерах. Если вы предоставите подробные инструкции в сочетании с примерами, это может помочь получить более точные результаты.

#### См. также {#see-also}

* [{#T}](./llm-settings.md)
* [{#T}](../../prompts/yandexgpt/index.md)
# Параметры LLM

Работа с LLM обычно происходит через API. При создании промтов может быть полезно настроить и протестировать несколько параметров, чтобы найти наиболее подходящие комбинации. Настройка этих параметров важна для улучшения стабильности и качества ответов. Вам потребуется провести несколько экспериментов, чтобы определить правильные настройки для решения ваших задач. Ниже приведены общие настройки, с которыми вы столкнетесь при использовании различных провайдеров LLM.

## Температура {#temperature}

Температура определяет степень случайности в ответах модели. Чем ниже температура, тем более детерминированными будут результаты, так как всегда выбирается наиболее вероятный следующий [токен](../../concepts/yandexgpt/tokens.md). Увеличение температуры может привести к большей случайности, что способствует более разнообразным или креативным ответам. На практике это означает, что для задач, требующих фактических и сжатых ответов, таких как вопросы и ответы на основе фактов, лучше использовать низкое значение температуры. Для генерации стихов или других творческих задач может быть полезно увеличить значение температуры.

## Максимальная длина {#max-length}

Вы можете управлять количеством токенов, которые генерирует модель, регулируя максимальную длину (параметр `max_length`). Указание максимальной длины помогает предотвратить длинные или нерелевантные ответы и контролировать затраты.

{% note info %}

Обратите внимание, что ответы моделей и результаты ваших экспериментов могут варьироваться в зависимости от версии используемой LLM.

{% endnote %}
---
title: "Основы работы с промтами"
description: "В статье содержатся основы работы с промтами {{ yagpt-name }}"
---

# Основы работы с промтами
 
*Промт* — это входные данные для генеративной модели, предназначенные для управления результатами ее работы. Промт может состоять из текста, картинки, звука или любой другой информации.

*Шаблон промта* — это незаконченный промт с одной или несколькими переменными, которые заменяются на какую-то информацию для создания отдельного экземпляра промта.

Например, нужно решить задачу бинарной классификации твитов. Исходный шаблон промта будет выглядеть так: 

```text
Классифицируй твит как «положительный» или «отрицательный»: {TWEET}.
```

Если в шаблон подставить твит из датасета, получится экземпляр промта, который можно отправить генеративной модели для предсказания результата.

Качество выходных данных модели зависит от того, насколько хорошо вы сформулируете запрос и сколько информации предоставите. Промт может содержать основные элементы: *инструкция* или *вопрос*, а также дополнительные: *контекст*, *входные данные* и *примеры*. Используя эти элементы, вы можете более эффективно управлять моделью и улучшать качество результатов.

Начнем с простого примера промта:

**Промт**

```text
Что такое замок?
```

**Ответ**

```text
Замок — это сложное и многофункциональное сооружение, которое обычно служит для защиты территории или имущества. Замки могут быть разных типов: средневековые, современные, военные, дворцовые и другие. Они могут включать в себя различные элементы, такие как стены, башни, ворота, рвы и мосты.

В прошлом замки были важными стратегическими объектами, которые защищали от врагов и позволяли контролировать территорию. Сегодня замки часто являются историческими памятниками и туристическими достопримечательностями.
```

В [консоли управления]({{ link-console-main }}) вы можете повторить эксперимент самостоятельно.

![Скриншот](../../../_assets/foundation-models/studybook/introduction/image.png)

### Структура промта {#structure}

Обращаясь к {{ yagpt-name }} через [API](../../concepts/api.md), вы можете структурировать промт с помощью ролей. {{ yagpt-full-name }} поддерживает следующие роли: `system`, `user` и `assistant`. В [промпт-режиме](../../concepts/yandexgpt/index.md) в консоли управления эти роли соответствуют полям **Инструкции**, **Запрос** и **Ответ**. Структурировать промпт не обязательно. 

Для простоты все дальнейшие примеры, если не указано иное, будут использовать только сообщение с ролью `user`.

Языковая модель отвечает последовательностью [токенов](../../concepts/yandexgpt/tokens.md), которые имеют смысл в контексте «замок». Однако результат может быть неожиданным или далеким от задачи, которую вы хотите выполнить. Например, мы хотели узнать про дверной замок. Этот простой пример подчеркивает необходимость предоставления большего контекста или инструкций о том, чего именно вы хотите добиться с помощью модели.

Попробуем дополнить промт:

**Промт**

```text
Что такое замок? (речь идет о дверных замках)
```

**Ответ**

```text
Замок — это механическое, электронное или комбинированное устройство фиксации двери в закрытом состоянии. Оно предотвращает несанкционированный доступ в помещение и обеспечивает безопасность.

Дверные замки различаются по конструкции, типу установки, уровню секретности и другим характеристикам. Они могут быть врезными, накладными, навесными и др.

Замки используются для защиты жилых домов, квартир, офисов, складов и других помещений.
```

В данном примере мы направлем модель в необходимую нам сторону, уточняя запрос. Это и есть суть промт-инжиниринга.

## Форматирование промтов {#format}

Стандартный промт имеет следующий формат:

```text
<Вопрос>?
```

или

```text
<Инструкция>
```
 
Вы можете преобразовать его в формат вопрос-ответ (QA), который является стандартным для многих задач:

```text
Вопрос: <Вопрос>?
Ответ:
```

Когда вы задаете промт таким образом, обычно считается, что вы используете *промтинг Zero-shot*, то есть вы напрямую спрашиваете модель без каких-либо примеров или демонстраций задачи, которую хотите выполнить. Некоторые языковые модели способны выполнять промтинг Zero-shot, но результат зависит от сложности и понимания задачи, а также от примеров, на которых модель была обучена.

### Few-shot prompting {#few-shot}

Одной из популярных и эффективных техник форматирования результата является *промтинг Few-shot*, в котором вы предоставляете примеры (демонстрации). Вы можете форматировать Few-shot-промты следующим образом:

```text
<Инструкция, как отвечать на вопрос>

<Вопрос>?
<Ответ>

<Вопрос>?
<Ответ>

<Вопрос>?
<Ответ>

<Вопрос>?
```

Версия в формате QA будет выглядеть так:

```text
<Инструкция, как отвечать на вопрос>
Примеры:

Вопрос: <Вопрос>?
Ответ: <Ответ>

Вопрос: <Вопрос>?
Ответ: <Ответ>

Вопрос: <Вопрос>?
Ответ: <Ответ>

Вопрос: <Вопрос>?
Ответ:
```

Формат Few-shot промтов зависит от задачи, не обязательно использовать привденный выше промпт 1 в 1. Например, таким образом можно сделать простейшую классификацию текстов:

**Промт**

```text
Это потрясающе! // Положительно
Это плохо! // Отрицательно
Вау, этот фильм был классный! // Положительно
Какое ужасное шоу! //
```

**Ответ**

```text
Отрицательно
```

Языковые модели способны обучаться задачам в контексте (In-Context Learning, ICL) на примерах с помощью Few-shot-промтов. Более подробную информацию о промтах Zero-shot и Few-shot см. в разделе [{#T}](../techniques/about.md).

#### См. также {#see-also}

* [{#T}](elements.md)
* [{#T}](general-tips.md)
* [{#T}](examples.md)
* [{#T}](llm-settings.md)
# Общие советы для создания промтов

Рассмотрим несколько советов, которые помогут получить лучший результат от модели {{ yagpt-name }}.

## Начинайте с простого {#start-simply}

Создание промтов — это итеративный процесс, требующий множества экспериментов для достижения оптимальных результатов. Начните с простых промтов и, по мере необходимости, добавляйте больше элементов и контекста. Это поможет избежать излишней сложности на начальном этапе.

Когда у вас есть большая задача, состоящая из множества подзадач, попробуйте разбить ее на более простые части и постепенно усложнять по мере получения лучших результатов.

## Используйте инструкцию {#instruction}

Для выполнения простых задач можно использовать такие команды, как «Напиши», «Классифицируй», «Суммируй», «Переведи», «Упорядочи» и т.д. Экспериментируйте с различными инструкциями, ключевыми словами, контекстами и данными, чтобы найти наилучший подход для вашей задачи.

Рекомендуется размещать инструкции в начале промта и использовать четкий разделитель, например «###», чтобы отделить инструкцию от контекста:

**Промт**

```text
### Инструкция ###
Переведи текст ниже на испанский язык:

Текст: «Привет!»
```

**Ответ**

```text
¡Hola!
```

## Конкретизируйте запросы {#specificity}

Чем более детализированным будет промт, тем лучше будут результаты. Это особенно важно, когда у вас есть желаемый результат или стиль генерации:

**Промт**

```text
Извлеки названия мест из следующего текста.

Желаемый формат:
Место: <список_мест_через_запятую>

Текст: "Хотя эти разработки обнадеживают исследователей, многое остается загадкой. «У нас часто есть черный ящик между мозгом и эффектом, который мы видим на периферии», — говорит Энрике Вейга-Фернандес, нейроиммунолог из Центра неизвестного в Лиссабоне. «Если мы хотим использовать это в терапевтическом контексте, нам действительно нужно понять механизм.»"
```

**Ответ**

```text
Место: Центр неизвестного, Лиссабон
```

## Избегайте неточностей {#inaccuracies}

Чем более четкие и конкретные инструкции вы зададите, тем эффективнее будет действовать модель. Используйте такие формулировки, как:

* объясни так, чтобы понял даже школьник;
* напиши в официально-деловом стиле;
* используй яркий художественный стиль.

## Что делать, а что не делать {#what-to-do}

Избегайте указаний на то, что не нужно делать, и сосредоточьтесь на том, что сделать нужно. Это способствует большей конкретности и фокусировке на деталях, которые положительно влияют на результат. Например:

**Промт**

```text
Следующий агент рекомендует фильмы клиенту. Агент должен рекомендовать фильм из списка топовых мировых трендов. Он должен воздерживаться от вопросов о предпочтениях пользователей и избегать запроса личной информации. Если агент не может порекомендовать фильм, он должен ответить: "Извините, сегодня не могу порекомендовать фильм."

Клиент: Порекомендуйте фильм на основе моих интересов.
Агент:
```

**Ответ**

```text
Извините, я не знаю ваших интересов. Однако вот список топовых мировых трендов: [список фильмов]. Надеюсь, вы найдете что-то по душе!
```

#### См. также {#see-also}

* [{#T}](elements.md)
* [{#T}](examples.md)
* [{#T}](llm-settings.md)
---
title: "Элементы промта"
description: "В статье содержится описание элементов промтов больших генеративных моделей"
---

# Элементы промта

![Схема элементов промта](../../../_assets/foundation-models/studybook/introduction/image-1.svg)

Промт может содержать следующие элементы:

* *Инструкция* — конкретная задача, требуемый стиль письма и другие предписания для модели.
* *Контекст* — внешняя информация или дополнительные уточнения, которые помогут получить лучший результат.
* *Входные данные* — данные для предсказаний или вопрос, на который мы хотим получить ответ.
* *Индикатор вывода* — тип или формат ожидаемых выходных данных.

Рассмотрим простой пример классификации текста, чтобы лучше понять элементы промта:

**Промт**

```text
Классифицируй текст как нейтральный, негативный или позитивный.

Текст: Я думаю, что еда была нормальной.

Настроение:
```

**Ответ**

```text
Нейтральное
```

В приведенном примере промта инструкция соответствует задаче классификации: «Классифицируй текст как нейтральный, негативный или позитивный». Входные данные представлены фразой «Я думаю, что еда была нормальной», а индикатор вывода — «Настроение:».

В этом примере не используется контекст, но его также можно добавить в промт. Например, контекст для этого промта классификации текста может включать примеры похожих вопросов и ответов, чтобы помочь модели лучше понять задачу и ожидаемые результаты. Также вы можете задать формат выходных данных, например, попросить модель заключить текст в кавычки или в формате JSON.

Не обязательно использовать все четыре элемента в каждом промте. Формат промта зависит от конкретной задачи.

#### См. также {#see-also}

* [{#T}](examples.md)
# Генерация с использованием поиска (RAG) для больших языковых моделей

Работа с большими языковыми моделями (LLM) сопряжена с рядом проблем, таких как пробелы в знаниях, вопросы фактичности и галлюцинации. Генерация с использованием поиска (RAG) предлагает решение для смягчения некоторых из этих проблем, дополняя LLM внешними источниками знаний, такими как базы данных. RAG особенно полезен в сценариях, требующих интенсивных знаний или в приложениях, где информация постоянно обновляется. Ключевым преимуществом RAG является то, что LLM не нужно переобучать для конкретных задач. RAG стал популярным благодаря своему применению в разговорных агентах.

В этом обзоре мы выделяем основные выводы и практические инсайты из недавнего исследования под названием [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997) (Gao et al., 2023). В частности, мы сосредоточимся на существующих подходах, передовых методах RAG, оценке, приложениях и технологиях, окружающих различные компоненты системы RAG (поиск, генерация и методы дополнения).

## Введение в RAG

![RAG Framework](image.png)

Как лучше всего объяснено [здесь](https://www.promptingguide.ai/techniques/rag), RAG можно определить следующим образом:

> RAG принимает входные данные и извлекает набор релевантных/поддерживающих документов из источника (например, Википедии). Документы объединяются в качестве контекста с исходным промптом и передаются генератору текста, который создает окончательный вывод. Это делает RAG адаптивным для ситуаций, когда факты могут изменяться со временем. Это очень полезно, так как параметрические знания LLM статичны. RAG позволяет языковым моделям обходить необходимость переобучения, обеспечивая доступ к последней информации для генерации надежных выводов через генерацию на основе поиска.

Вкратце, извлеченные доказательства, полученные в RAG, могут служить способом повышения точности, управляемости и релевантности ответа LLM. Это помогает уменьшить проблемы галлюцинаций или производительности при решении задач в быстро меняющейся среде.

Хотя RAG также включал оптимизацию методов предобучения, текущие подходы в значительной степени перешли к сочетанию сильных сторон RAG и мощных дообученных моделей, таких как Yandex GPT. На диаграмме ниже показана эволюция исследований, связанных с RAG:

![RAG Framework](image-1.svg)
*[Источник рисунка](https://arxiv.org/abs/2312.10997)*

Ниже приведен типичный рабочий процесс приложения RAG:

![RAG Framework](image-2.svg)
*[Источник рисунка](https://arxiv.org/abs/2312.10997)*

Мы можем объяснить различные шаги/компоненты следующим образом:
- **Входные данные:** Вопрос, на который система LLM отвечает, называется входными данными. Если RAG не используется, LLM напрямую отвечает на вопрос.
- **Индексация:** Если используется RAG, то серия связанных документов индексируется путем их разбиения на части, создания эмбеддингов этих частей и индексации их в векторном хранилище. При выводе запрос также преобразуется в эмбеддинг аналогичным образом.
- **Поиск:** Релевантные документы извлекаются путем сравнения запроса с индексированными векторами, также называемыми "релевантными документами".
- **Генерация:** Релевантные документы объединяются с исходным промптом в качестве дополнительного контекста. Объединенный текст и промпт затем передаются модели для генерации ответа, который затем подготавливается как окончательный вывод системы для пользователя.

В приведенном примере использование модели напрямую не позволяет ответить на вопрос из-за отсутствия знаний о текущих событиях. С другой стороны, при использовании RAG система может получить релевантную информацию, необходимую для правильного ответа на вопрос.

## Парадигмы RAG

За последние несколько лет системы RAG эволюционировали от наивного RAG до продвинутого RAG и модульного RAG. Эта эволюция произошла для решения определенных ограничений, связанных с производительностью, стоимостью и эффективностью.

![RAG Framework](image-3.svg)
*[Источник рисунка](https://arxiv.org/abs/2312.10997)*

### Наивный RAG
Наивный RAG следует традиционному процессу индексации, поиска и генерации. Вкратце, пользовательский ввод используется для запроса релевантных документов, которые затем объединяются с промптом и передаются модели для генерации окончательного ответа. История диалога может быть интегрирована в промпт, если приложение включает многократные диалоговые взаимодействия.

Наивный RAG имеет ограничения, такие как низкая точность (несоответствие извлеченных частей) и низкая полнота (неспособность извлечь все релевантные части). Также возможно, что LLM передается устаревшая информация, что является одной из основных проблем, которые система RAG должна изначально решать. Это приводит к проблемам галлюцинаций и плохим и неточным ответам.

Когда применяется дополнение, могут возникнуть проблемы с избыточностью и повторением. При использовании нескольких извлеченных фрагментов также важны ранжирование и согласование стиля/тона. Еще одна проблема заключается в том, чтобы генерация не слишком зависела от дополненной информации, что может привести к тому, что модель просто повторяет извлеченное содержание.

### Продвинутый RAG

Продвинутый RAG помогает решать проблемы, присутствующие в наивном RAG, такие как улучшение качества поиска, которое может включать оптимизацию процессов до поиска, поиска и после поиска.

Процесс до поиска включает оптимизацию индексации данных, которая направлена на повышение качества индексируемых данных через пять этапов: повышение гранулярности данных, оптимизация структур индекса, добавление метаданных, оптимизация выравнивания и смешанный поиск.

Этап поиска можно дополнительно улучшить, оптимизируя саму модель эмбеддинга, что напрямую влияет на качество частей, составляющих контекст. Это можно сделать путем дообучения эмбеддинга для оптимизации релевантности поиска или использования динамических эмбеддингов, которые лучше улавливают контекстное понимание (например, модель embeddings-ada-02 от OpenAI).

Оптимизация после поиска фокусируется на избегании ограничений окна контекста и работе с шумной или потенциально отвлекающей информацией. Общий подход к решению этих проблем — повторное ранжирование, которое может включать такие подходы, как перемещение релевантного контекста к краям промпта или пересчет семантического сходства между запросом и релевантными текстовыми фрагментами. Сжатие промпта также может помочь в решении этих проблем.

### Модульный RAG

Как следует из названия, модульный RAG улучшает функциональные модули, такие как включение модуля поиска для поиска по сходству и применение дообучения в поисковике. И наивный RAG, и продвинутый RAG являются частными случаями модульного RAG и состоят из фиксированных модулей. Расширенные модули RAG включают поиск, память, слияние, маршрутизацию, предсказание и адаптер задач, которые решают различные проблемы. Эти модули можно перестраивать в соответствии с конкретными задачами. Таким образом, модульный RAG выигрывает от большей разнообразности и гибкости, так как вы можете добавлять или заменять модули или настраивать поток между модулями в зависимости от требований задачи.

С учетом увеличенной гибкости в построении систем RAG, были предложены другие важные методы оптимизации для оптимизации конвейеров RAG, включая:

- **Гибридное исследование поиска:** Этот подход использует комбинацию методов поиска, таких как поиск по ключевым словам и семантический поиск, для извлечения релевантной и контекстно богатой информации; это полезно при работе с различными типами запросов и информационными потребностями.
- **Рекурсивный поиск и движок запросов:** Включает рекурсивный процесс поиска, который может начинаться с небольших семантических фрагментов и последовательно извлекать более крупные фрагменты, обогащающие контекст; это полезно для балансировки эффективности и контекстно богатой информации.
- **StepBack-промпт:** [Техника промптинга](https://arxiv.org/abs/2310.06117), которая позволяет LLM выполнять абстракцию, создавая концепции и принципы, которые направляют рассуждения; это приводит к более обоснованным ответам при использовании в рамках RAG, так как LLM отходит от конкретных примеров и может рассуждать более широко, если это необходимо.
- **Подзапросы:** Существуют различные стратегии запросов, такие как древовидные запросы или последовательный запрос фрагментов, которые можно использовать для различных сценариев. LlamaIndex предлагает [движок подзапросов](https://docs.llamaindex.ai/en/latest/understanding/putting_it_all_together/agents.html#), который позволяет разбить запрос на несколько вопросов, использующих различные релевантные источники данных.
- **Гипотетические эмбеддинги документов:** [HyDE](https://arxiv.org/abs/2212.10496) генерирует гипотетический ответ на запрос, создает его эмбеддинг и использует его для поиска документов, похожих на гипотетический ответ, вместо использования самого запроса.

## Архитектура RAG

В этом разделе мы обобщаем ключевые разработки компонентов системы RAG, которые включают поиск, генерацию и дополнение.

### Поиск

Поиск — это компонент RAG, который занимается извлечением высоко релевантного контекста из поисковика. Поисковик можно улучшить различными способами, включая:

**Улучшение семантических представлений**

Этот процесс включает прямое улучшение семантических представлений, которые питают поисковик. Вот несколько соображений:

- **Разбиение на части:** Один из важных шагов — выбор правильной стратегии разбиения, которая зависит от содержания, с которым вы работаете, и приложения, для которого вы генерируете ответы. Разные модели также демонстрируют разные сильные стороны на различных размерах блоков. Трансформеры предложений будут лучше работать с отдельными предложениями, но text-embedding-ada-002 будет лучше работать с блоками, содержащими 256 или 512 токенов. Другие аспекты, которые следует учитывать, включают длину пользовательских вопросов, приложение и ограничения по токенам, но обычно экспериментируют с различными стратегиями разбиения, чтобы помочь оптимизировать поиск в вашей системе RAG.
- **Дообученные модели эмбеддингов:** После того как вы определили эффективную стратегию разбиения, может потребоваться дообучение модели эмбеддингов, если вы работаете с специализированной областью. В противном случае возможно, что пользовательские запросы будут полностью неправильно поняты в вашем приложении. Вы можете дообучить на широких знаниях области (т.е. дообучение знаний области) и для конкретных задач. [BGE-large-EN, разработанная BAAI](https://github.com/FlagOpen/FlagEmbedding), является заметной моделью эмбеддингов, которую можно дообучить для оптимизации релевантности поиска.

**Выравнивание запросов и документов**

Этот процесс занимается выравниванием пользовательских запросов с документами в семантическом пространстве. Это может потребоваться, когда пользовательский запрос может не содержать семантической информации или содержать неточные формулировки. Вот некоторые подходы:

- **Переписывание запросов:** Фокусируется на переписывании запросов с использованием различных техник, таких как [Query2Doc](https://arxiv.org/abs/2303.07678), [ITER-RETGEN](https://arxiv.org/abs/2305.15294) и HyDE.
- **Трансформация эмбеддингов:** Оптимизирует представление эмбеддингов запросов и выравнивает их с латентным пространством, которое более тесно связано с задачей.

**Выравнивание поисковика и LLM**

Этот процесс занимается выравниванием выходных данных поисковика с предпочтениями LLM.

- **Дообучение поисковиков:** Использует сигналы обратной связи LLM для уточнения моделей поиска. Примеры включают адаптированный к дополнению поисковик ([AAR](https://arxiv.org/abs/2305.17331)), [REPLUG](https://arxiv.org/abs/2301.12652) и [UPRISE](https://arxiv.org/abs/2303.08518), чтобы назвать несколько.
- **Адаптеры:** Включает внешние адаптеры для помощи в процессе выравнивания. Примеры включают [PRCA](https://aclanthology.org/2023.emnlp-main.326/), [RECOMP](https://arxiv.org/abs/2310.04408) и [PKG](https://arxiv.org/abs/2305.04757).

### Генерация

Генератор в системе RAG отвечает за преобразование извлеченной информации в связный текст, который будет формировать окончательный вывод модели. Этот процесс включает разнообразные входные данные, которые иногда требуют усилий для уточнения адаптации языковой модели к входным данным, полученным из запросов и документов. Это можно решить с помощью процесса после поиска и дообучения:

- **Процесс после поиска с замороженной LLM:** Обработка после поиска оставляет LLM нетронутой и вместо этого фокусируется на улучшении качества результатов поиска через операции, такие как сжатие информации и повторное ранжирование результатов. Сжатие информации помогает уменьшить шум, решить проблемы с ограничениями длины контекста LLM и улучшить эффекты генерации. Повторное ранжирование направлено на переупорядочивание документов, чтобы приоритет был отдан наиболее релевантным элементам.
- **Дообучение LLM для RAG:** Для улучшения системы RAG генератор можно дополнительно оптимизировать или дообучить, чтобы гарантировать, что сгенерированный текст является естественным и эффективно использует извлеченные документы.

### Дополнение

Дополнение включает процесс эффективной интеграции контекста из извлеченных фрагментов с текущей задачей генерации. Прежде чем обсуждать больше о процессе дополнения, этапах дополнения и данных дополнения, вот таксономия основных компонентов RAG:

![RAG Taxonomy](image-4.svg)
*[Источник рисунка](https://arxiv.org/abs/2312.10997)*

Дополнение поиска можно применять на различных этапах, таких как предобучение, дообучение и вывод.

- **Этапы дополнения:** [RETRO](https://arxiv.org/abs/2112.04426) является примером системы, которая использует дополнение поиска для крупномасштабного предобучения с нуля; она использует дополнительный кодировщик, построенный на основе внешних знаний. Дообучение также можно комбинировать с RAG для разработки и улучшения эффективности систем RAG. На этапе вывода применяются многие техники для эффективного включения извлеченного контента для удовлетворения специфических требований задачи и дальнейшего уточнения процесса RAG.

- **Источник дополнения:** Эффективность модели RAG сильно зависит от выбора источника данных для дополнения. Данные можно классифицировать на неструктурированные, структурированные и сгенерированные LLM данные.

- **Процесс дополнения:** Для многих задач (например, многократного рассуждения) одного поиска недостаточно, поэтому были предложены несколько методов:
    - **Итеративный поиск** позволяет модели выполнять несколько циклов поиска для улучшения глубины и релевантности информации. Известные подходы, использующие этот метод, включают [RETRO](https://arxiv.org/abs/2112.04426) и [GAR-meets-RAG](https://arxiv.org/abs/2310.20158).
    - **Рекурсивный поиск** рекурсивно использует вывод одного этапа поиска в качестве входных данных для другого этапа поиска; это позволяет углубляться в релевантную информацию для сложных и многократных запросов (например, академические исследования и анализ юридических дел). Известные подходы, использующие этот метод, включают [IRCoT](https://arxiv.org/abs/2212.10509) и [Tree of Clarifications](https://arxiv.org/abs/2310.14696).
    - **Адаптивный поиск** настраивает процесс поиска в соответствии с конкретными требованиями, определяя оптимальные моменты и содержание для поиска. Известные подходы, использующие этот метод, включают [FLARE](https://arxiv.org/abs/2305.06983) и [Self-RAG](https://arxiv.org/abs/2310.11511).

На рисунке ниже представлено подробное описание исследований RAG с различными аспектами дополнения, включая этапы дополнения, источник и процесс.

![RAG Augmentation Aspects](image-5.svg)
*[Источник рисунка](https://arxiv.org/abs/2312.10997)*

### RAG vs. Дообучение

Существует множество открытых дискуссий о различиях между RAG и дообучением и в каких сценариях каждый из них уместен. Исследования в этих двух областях показывают, что RAG полезен для интеграции новых знаний, в то время как дообучение можно использовать для улучшения производительности и эффективности модели путем улучшения внутренних знаний, формата вывода и обучения сложным инструкциям. Эти подходы не являются взаимоисключающими и могут дополнять друг друга в итеративном процессе, направленном на улучшении производительности модели.

## Оценка RAG

Оценка систем RAG является важным аспектом, так как она помогает понять, насколько эффективно система решает задачи и насколько она улучшает производительность по сравнению с другими подходами. Оценка RAG включает несколько метрик и методов, которые помогают измерить различные аспекты системы, такие как точность поиска, качество генерации и фактичность ответов.

### Метрики оценки

1. **Точность поиска**:
   - **Precision@k**: Процент релевантных документов среди первых \(k\) извлеченных документов.
   - **Recall@k**: Процент всех релевантных документов, которые были извлечены среди первых \(k\) документов.
   - **Mean Reciprocal Rank (MRR)**: Среднее значение обратного ранга первого релевантного документа.

2. **Качество генерации**:
   - **BLEU (Bilingual Evaluation Understudy)**: Метрика, измеряющая точность машинного перевода путем сравнения с эталонными переводами.
   - **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Метрика, измеряющая качество резюмирования текста путем сравнения с эталонными резюме.
   - **METEOR (Metric for Evaluation of Translation with Explicit ORdering)**: Метрика, учитывающая точность, полноту и порядок слов.

3. **Фактичность ответов**:
   - **FactCC**: Метрика, оценивающая фактическую корректность сгенерированных текстов.
   - **FEQA (Fact Extraction and Question Answering)**: Метрика, оценивающая фактическую корректность путем извлечения фактов и проверки их с помощью вопросов и ответов.

### Методы оценки

1. **Автоматическая оценка**: Использование вышеупомянутых метрик для автоматической оценки качества системы. Это позволяет быстро и эффективно измерять производительность системы на больших объемах данных.

2. **Человеческая оценка**: Включает привлечение экспертов или пользователей для оценки качества и фактичности сгенерированных ответов. Это позволяет получить более точную и детализированную оценку, особенно в сложных или неоднозначных случаях.

3. **А/Б тестирование**: Сравнение двух версий системы (например, с использованием RAG и без него) на реальных пользователях для оценки улучшений в производительности и удовлетворенности пользователей.

### Применения RAG

RAG находит применение в различных областях, включая:

1. **Разговорные агенты**: Улучшение качества ответов и уменьшение количества галлюцинаций в чат-ботах и виртуальных помощниках.
2. **Системы вопрос-ответ**: Повышение точности и полноты ответов на вопросы, требующие актуальной информации.
3. **Резюмирование текста**: Создание более точных и информативных резюме больших текстов.
4. **Машинный перевод**: Улучшение качества перевода путем интеграции актуальной информации из внешних источников.
5. **Анализ данных и бизнес-аналитика**: Помощь в извлечении и интерпретации данных из больших объемов информации.

### Заключение

Генерация с использованием поиска (RAG) представляет собой мощный инструмент для улучшения производительности и качества работы больших языковых моделей. Эволюция RAG от наивного к продвинутому и модульному подходам позволяет более гибко настраивать систему в зависимости от конкретных задач. Оценка RAG с использованием различных метрик и методов помогает понять, насколько эффективно система решает задачи и насколько она улучшает производительность по сравнению с другими подходами. Применение RAG в различных областях, таких как разговорные агенты, системы вопрос-ответ, резюмирование текста и машинный перевод, демонстрирует его потенциал для создания более точных и информативных систем.







# Архитектуры сервисов с использованием LLM

Промт-инжиниринг позволяет добиваться требуемых результатов. Техники позволяют улучшать работу этих промптов. В этой главе мы немного отойдем от работы только с промптами и посмотрим, как можно получать требуемые результаты с помощью LLM более глобально - строя сложные пайплайны обработки данных и используя другие сервисы Yandex Cloud.

# Страницы в этой главе

* [{#T}](./rag.md)
---
title: "Техника работы с промтами Chain-of-Thought"
description: "В статье содержатся примеры работы с разными видами промтинга техники Chain-of-Thought."
---

# Промтинг Chain-of-Thought (CoT)

![cot](../../../_assets/foundation-models/studybook/techniques/image-1.svg)

Промтинг Chain-of-Thought (CoT), представленный в работе [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), позволяет моделям типа {{ yagpt-name }} выполнять сложные задачи, требующие промежуточных шагов рассуждения. Этот метод можно комбинировать с промтами [Few-shot](zero-shot.md), чтобы улучшить результаты на сложных задачах. Рассмотрим простой арифметический пример:

**Промт**

```text
Нечетные числа в этой группе дают в сумме четное число: 4, 8, 9, 15, 12, 2, 1.
A: Сложив все нечетные числа (9, 15, 1), получаем 25. Ответ: Ложь.

Нечетные числа в этой группе дают в сумме четное число: 16, 11, 14, 4, 8, 13, 24.
A: Сложив все нечетные числа (11, 13), получаем 24. Ответ: Истина.

Нечетные числа в этой группе дают в сумме четное число: 17, 10, 19, 4, 8, 12, 24.
A: Сложив все нечетные числа (17, 19), получаем 36. Ответ: Истина.

Нечетные числа в этой группе дают в сумме четное число: 17, 9, 10, 12, 13, 4, 2.
A: Сложив все нечетные числа (17, 9, 13), получаем 39. Ответ: Ложь.

Нечетные числа в этой группе дают в сумме четное число: 15, 32, 13, 82, 7, 1.
A:
```

**Ответ**

```text
Сложив все нечётные числа (15, 13, 7, 1), получаем 36. Ответ: Истина.
```

Мы видим, что результат идеален, когда мы предоставляем шаги рассуждения. На самом деле для решения этой задачи достаточно даже одного примера:

**Промт**

```
Нечетные числа в этой группе дают в сумме четное число: 4, 8, 9, 15, 12, 2, 1.
A: Сложив все нечетные числа (9, 15, 1), получаем 25. Ответ: Ложь.

Нечетные числа в этой группе дают в сумме четное число: 15, 32, 5, 13, 82, 7, 1.
A:
```

**Ответ**

```
Сложив все нечетные числа (15, 5, 13, 7, 1), получаем 41. Ответ: Ложь.
```

Авторы техники утверждают, что такой способностью обладают только достаточно большие языковые модели.

## Промтинг Zero-shot CoT {#zero-shot-cot}

![zero-cot](../../../_assets/foundation-models/studybook/techniques/image-2.svg)

Промтинг Zero-shot CoT, представленный в работе [Kojima et al. (2022)](https://arxiv.org/abs/2205.11916), предполагает добавление фразы «Давайте подумаем шаг за шагом» к исходному промту. Попробуем решить простую задачу:

**Промт**

```
Я пошел на рынок и купил 10 яблок. Я отдал 2 яблока соседу и 2 ремонтнику. Затем я купил еще 5 яблок и съел 1. Сколько яблок у меня осталось?
```

**Ответ**

```
11 яблок
```

Ответ неверный. Попробуем использовать технику zero-shot CoT промтинга:

**Промт**

```
Я пошел на рынок и купил 10 яблок. Я отдал 2 яблока соседу и 2 ремонтнику. Затем я купил еще 5 яблок и съел 1. Сколько яблок у меня осталось?

Давайте подумаем шаг за шагом.
```

**Ответ**

```
Сначала у вас было 10 яблок.
Вы отдали 2 яблока соседу и 2 ремонтнику, так что у вас осталось 6 яблок.
Затем вы купили еще 5 яблок, так что теперь у вас 11 яблок.
Наконец, вы съели 1 яблоко, так что у вас осталось 10 яблок.
```

Такая простая инструкция сильно повысила точность ответа. Это особенно полезно, когда у вас нет большого количества примеров для использования в промте.

## Автоматический Chain-of-Thought {#auto-cot}

Для эффективного промтинга Chain-of-Thought требуется ручное создание разнообразных примеров. Это большой труд, который может не окупиться. Авторы работы [Zhang et al. (2022)](https://arxiv.org/abs/2210.03493) предлагают подход, который устраняет ручные усилия, используя LLM с промтом «Давайте подумаем шаг за шагом» для генерации примеров цепочек рассуждений. Этот автоматический процесс все еще может приводить к ошибкам в сгенерированных цепочках, поэтому важно также разнообразие примеров.

Auto-CoT состоит из двух основных этапов:

1. Кластеризация вопросов: разделение вопросов из заданного набора данных на несколько кластеров.
1. Выбор примеров: выбор представительного вопроса из каждого кластера и генерация его цепочки рассуждений с использованием Zero-Shot-CoT с простыми эвристиками.

Простые эвристики могут включать длину вопросов (например, 60 [токенов](../../concepts/yandexgpt/tokens.md)) и количество шагов в рассуждении (например, 5 шагов рассуждения). Это побуждает модель использовать простые и точные демонстрации.

Процесс проиллюстрирован ниже:

![auto-cot](../../../_assets/foundation-models/studybook/techniques/image-5.svg)

Код для Auto-CoT доступен на [GitHub](https://github.com/amazon-science/auto-cot).

#### См. также {#see-also}

* [{#T}](few-shot.md)
* [{#T}](../../tutorials/yagpt-tuning.md)
# Самосогласованность

Одной из продвинутых техник для создания промтов является самосогласованность (self-consistency). Эта техника была предложена в работе [Wang et al. (2022)](https://arxiv.org/abs/2203.11171) в качестве замены «жадного» декодирования, используемого в цепочках рассуждений ([CoT](CoT.md)). Идея заключается в том, чтобы сэмплировать несколько разнообразных путей рассуждений через Few-shot CoT и использовать эти генерации для выбора наиболее согласованного ответа. Это помогает улучшить производительность CoT-промтов в задачах, связанных с арифметическими и логическими рассуждениями.

Рассмотрим пример арифметических рассуждений:

**Промт**
```
Когда мне было 6 лет, моя сестра была вдвое младше меня. Сейчас мне 70 лет. Сколько лет моей сестре?
```

**Ответ**

```
35
```

Это неверный ответ. Попробуем улучшить промт с помощью самосогласованности, показав модели примеры рассуждений и повторив запрос несколько раз:

**Промт**

```
В: В роще было 15 деревьев. Работники рощи посадят деревья сегодня. После этого в роще будет 21 дерево. Сколько деревьев посадили работники рощи сегодня?
О: Мы начинаем с 15 деревьев. Позже у нас будет 21 дерево. Разница должна быть количеством посаженных деревьев. Значит, они посадили 21 - 15 = 6 деревьев. Ответ: 6.

В: Если на парковке было 3 машины и приехали еще 2, сколько машин на парковке?
О: На парковке уже было 3 машины. Приехали еще 2. Теперь на парковке 3 + 2 = 5 машин. Ответ: 5.

В: У Леи было 32 шоколадки, а у ее сестры 42. Если они съели 35, сколько шоколадок у них осталось?
О: У Леи было 32 шоколадки, а у ее сестры 42. Значит, изначально было 32 + 42 = 74 шоколадки. Съели 35. Значит, у них осталось 74 - 35 = 39 шоколадок. Ответ: 39.

В: У Джейсона было 20 леденцов. Он отдал Денни несколько леденцов. Теперь у Джейсона 12 леденцов. Сколько леденцов Джейсон отдал Денни?
О: У Джейсона было 20 леденцов. Так как у него осталось 12, он отдал Денни 20 - 12 = 8 леденцов. Ответ: 8.

В: У Шона было пять игрушек. На Рождество он получил по две игрушки от мамы и папы. Сколько игрушек у него теперь?
О: У него было 5 игрушек. Он получил 2 от мамы, значит, у него стало 5 + 2 = 7 игрушек. Затем он получил еще 2 от папы, значит, у него теперь 7 + 2 = 9 игрушек. Ответ: 9.

В: В серверной было девять компьютеров. Каждый день с понедельника по четверг устанавливали по пять компьютеров. Сколько компьютеров теперь в серверной?
О: С понедельника по четверг прошло 4 дня. Каждый день добавляли по 5 компьютеров. Значит, всего добавили 4 * 5 = 20 компьютеров. Изначально было 9 компьютеров, значит, теперь их 9 + 20 = 29. Ответ: 29.

В: У Майкла было 58 мячей для гольфа. Во вторник он потерял 23 мяча. В среду он потерял еще 2. Сколько мячей у него осталось в конце среды?
О: Изначально у Майкла было 58 мячей. Во вторник он потерял 23, значит, у него осталось 58 - 23 = 35 мячей. В среду он потерял еще 2, значит, у него осталось 35 - 2 = 33 мяча. Ответ: 33.

В: У Оливии было 23 доллара. Она купила пять бубликов по 3 доллара каждый. Сколько денег у нее осталось?
О: Она купила 5 бубликов по 3 доллара каждый. Это значит, что она потратила 15 долларов. У нее осталось 23 - 15 = 8 долларов.

В: Когда мне было 6 лет, моя сестра была вдвое младше меня. Сейчас мне 70 лет. Сколько лет моей сестре?
О:
```

**Ответ 1**

```
Когда мне было 6 лет, моя сестра была вдвое младше меня, значит, ей было 3 года. Сейчас мне 70 лет, значит, ей 70 - 3 = 67 лет. Ответ: 67.
```

**Ответ 2**

```
Когда рассказчику было 6 лет, его сестра была вдвое младше, то есть ей было 3 года. Сейчас рассказчику 70 лет, значит, его сестре 70 - 3 = 67 лет. Ответ: 67.
```

**Ответ 3**

```
Когда мне было 6 лет, моя сестра была вдвое младше меня, значит, ей было 3 года. Сейчас мне 70 лет, значит, ей 70 / 2 = 35 лет. Ответ: 35.
```

Мы получили три разных ответа, один из которых неверный. Следующим шагом самосогласованности будет агрегация и анализ ответов, направленный на выявление самого популярного, который и станет окончательным. В нашем случае — это 67.
# Промтинг Zero-Shot

Большие языковые модели (LLM), такие как {{ yagpt-name }}, настроены на выполнение инструкций и обучены на больших объемах данных. Масштабное обучение дает использовать модели в режиме Zero-shot, который предполагает, что имеющих у модели данных достаточно для решения определенных задач. Рассмотрим простой пример классификации текста с помощью техники Zero-shot.

**Промт**

```text
Классифицируйте текст как нейтральный, негативный или позитивный.

Текст: Я думаю, что отпуск был нормальным.
Настроение:
```

**Ответ**
```text
Нейтральный
```

В этом промте мы не предоставили модели никаких примеров текста с классификациями, но модель уже понимает, что такое «настроение».

Расширить возможности промтинга Zero-shot можно с помощью [настройки инструкций](https://arxiv.org/pdf/2109.01652) — дообучения моделей на наборах данных, описанных через инструкции. Также можно использовать [RLHF](https://arxiv.org/abs/1706.03741) — обучение с подкреплением на основе обратной связи от человека — для масштабирования настройки инструкций, где модель лучше подстраивается под человеческие предпочтения.

#### См. также {#see-also}

* [{#T}](few-shot.md)
* [{#T}](CoT.md)
* [{#T}](self-consistency.md)
* [{#T}](../../tutorials/yagpt-tuning.md)

---
title: "Техника работы с промтами Few-Shot"
description: "В статье содержатся примеры работы промтинга с помощью техники Few-Shot."
---

# Промтинг Few-Shot

![few-shot](../../../_assets/foundation-models/studybook/techniques/image-3.svg)

Хотя крупные языковые модели демонстрируют отличные способности в режиме [Zero-shot](zero-shot.md), они все еще не справляются со сложными задачами. Промтинг Few-shot промтинг можно использовать как технику для обучения в контексте, где для повышения качества результатов модели предоставляются примеры, на которых она обучается.

Согласно [Touvron et al. 2023](https://arxiv.org/pdf/2302.13971.pdf), свойства Few-shot впервые появились, когда модели были масштабированы до достаточного размера [(Kaplan et al., 2020)](https://arxiv.org/abs/2001.08361).

Рассмотрим пример из работы [Brown et al. 2020](https://arxiv.org/abs/2005.14165), в котором задача заключается в правильном использовании нового слова в предложении:

**Промт**

```text
"Котопес" — это мифическое существо, которое является наполовину котом, наполовину собакой. Пример предложения, использующего слово "котопес":
Мы гуляли по лесу и увидели странного котопса.

"Фырчать" означает издавать короткие, резкие звуки. Пример предложения, использующего слово "фырчать":
```

**Ответ**

```text
Когда мы подошли ближе, котопес начал фырчать на нас.
```

Модель научилась выполнять задачу, хотя имела всего один пример (т.е. 1-shot). Для более сложных задач можно экспериментировать с увеличением количества примеров (например, 3-shot, 5-shot, 10-shot и т.д.).

Из работы [Min et al. (2022)](https://arxiv.org/abs/2202.12837) следует, что для составления промтов Few-shot важны даже случайные, неправильные примеры. 

![probability](../../../_assets/foundation-models/studybook/techniques/image-4.svg)

Рассмотрим промт с неправильными примерами, в котором метки «Негативный» и «Позитивный» случайным образом присваиваются входным данным:

**Промт**
```text
Это потрясающе! // Негативный
Это плохо! // Позитивный
Вау, этот фильм был классный! // Позитивный
Какое ужасное шоу! //
```

**Ответ**
```text
Негативный
```

Мы все равно получаем верный ответ и формат, хотя метки были случайными. Также эксперименты подтверждают, что новые модели {{ yagpt-name }} становятся более устойчивыми к формату входных данных:

**Промт**
```text
Позитивный Это потрясающе! 
Это плохо! Негативный
Вау, этот фильм был классный!
Позитивный
Какое ужасное шоу! --
```

**Ответ**
```text
Негативный
```

Каждый пример из промта имеет разный формат, но модель все равно указала правильную метку. Однако, для сложных задач и разных вариаций промтов требуется более тщательный анализ.

{% note info %}

При использовании промтов Few-shot в слишком сложных задачах модель может переобучиться. Это значит, что она случайным образом запомнит ответ на какой-либо вопрос и будет отвечать так в любых ситуациях.

{% endnote %}

## Ограничения промтов Few-shot {#few-shot-limits}

Стандартный промтинг Few-shot хорошо работает для многих задач, но все же не является идеальной техникой, особенно при работе со сложными задачами, требующими рассуждений. Рассмотрим пример из арифметики:

**Промт**

```text
Нечетные числа в этой группе складываются в четное число: 15, 32, 5, 13, 82, 7, 1. 

Ответ: 
```

**Ответ**

```
Да, нечетные числа в этой группе складываются в 107, что является четным числом.
```

Добавим несколько примеров, чтобы увидеть, улучшит ли результаты Few-shot-промт:

**Промт**

```text
Нечетные числа в этой группе складываются в четное число: 4, 8, 9, 15, 12, 2, 1.
Ответ: Ответ — Ложь.

Нечетные числа в этой группе складываются в четное число: 17, 10, 19, 4, 8, 12, 24.
Ответ: Ответ — Правда.

Нечетные числа в этой группе складываются в четное число: 16, 11, 14, 4, 8, 13, 24.
Ответ: Ответ — Правда.

Нечетные числа в этой группе складываются в четное число: 17, 9, 10, 12, 13, 4, 2.
Ответ: Ответ — Ложь.

Нечетные числа в этой группе складываются в четное число: 15, 32, 5, 13, 82, 7, 1. 
Ответ: 
```

**Ответ**

```text
Ответ — Правда.
```

Это неверный ответ. Кажется, что промта Few-shot недостаточно, чтобы получить надежный результат в задачах рассуждения. [Техника CoT](CoT.md) справляется с такими задачами лучше.

Если промтов Zero-shot и Few-shot недостаточно, возможно, модель не обладает достаточными знаниями для решения задачи. В этом случае рекомендуем попробовать [дообучить](../../tutorials/yagpt-tuning.md) модель или поэкспериментировать с другими техниками промтинга.

#### См. также {#see-also}

* [{#T}](CoT.md)
* [{#T}](self-consistency.md)
* [{#T}](../../tutorials/yagpt-tuning.md)

# Техники работы с промтами

Промт-инжиниринг помогает эффективно разрабатывать и улучшать промты для получения лучших результатов при выполнении различных задач с использованием больших языковых моделей.

В этом разделе мы рассмотрим более продвинутые техники промт-инжиниринга, которые позволяют решать сложные задачи и улучшать надежность и производительность LLM:

* [{#T}](./zero-shot.md)
* [{#T}](./few-shot.md)
* [{#T}](./CoT.md)
* [{#T}](./self-consistency.md)
